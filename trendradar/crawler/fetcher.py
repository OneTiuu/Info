# coding=utf-8
"""
æ•°æ®è·å–å™¨æ¨¡å—

è´Ÿè´£ä» NewsNow API æŠ“å–æ–°é—»æ•°æ®ï¼Œæ”¯æŒï¼š
- å•ä¸ªå¹³å°æ•°æ®è·å–
- æ‰¹é‡å¹³å°æ•°æ®çˆ¬å–
- è‡ªåŠ¨é‡è¯•æœºåˆ¶
- ä»£ç†æ”¯æŒ
"""
from .local_adapters import LocalAdapters, ADAPTER_MAP
import json
import random
import time
from typing import Dict, List, Tuple, Optional, Union

import requests


class DataFetcher:
    """æ•°æ®è·å–å™¨"""

    # é»˜è®¤ API åœ°å€
    DEFAULT_API_URL = "https://newsnow.busiyi.world/api/s"

    # é»˜è®¤è¯·æ±‚å¤´
    DEFAULT_HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Connection": "keep-alive",
        "Cache-Control": "no-cache",
    }

    def __init__(
        self,
        proxy_url: Optional[str] = None,
        api_url: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–æ•°æ®è·å–å™¨
        
        Args:
            proxy_url: ä»£ç†æœåŠ¡å™¨ URLï¼ˆå¯é€‰ï¼‰
            api_url: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ DEFAULT_API_URLï¼‰
        """
        self.proxy_url = proxy_url
        self.api_url = api_url or self.DEFAULT_API_URL
        self.local_adapters = LocalAdapters()
    def fetch_data(
        self,
        id_info: Union[str, Tuple[str, str]],
        max_retries: int = 2,
        min_retry_wait: int = 3,
        max_retry_wait: int = 5,
    ) -> Tuple[Optional[str], str, str]:
        """
        è·å–æŒ‡å®šIDæ•°æ®ï¼Œæ”¯æŒé‡è¯•

        Args:
            id_info: å¹³å°ID æˆ– (å¹³å°ID, åˆ«å) å…ƒç»„
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            min_retry_wait: æœ€å°é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            max_retry_wait: æœ€å¤§é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            (å“åº”æ–‡æœ¬, å¹³å°ID, åˆ«å) å…ƒç»„ï¼Œå¤±è´¥æ—¶å“åº”æ–‡æœ¬ä¸º None
        """
        if isinstance(id_info, tuple):
            id_value, alias = id_info
        else:
            id_value = id_info
            alias = id_value
        if id_value in ADAPTER_MAP:
            method_name = ADAPTER_MAP[id_value]
            adapter_func = getattr(self.local_adapters, method_name)
            
            print(f">>> æ­£åœ¨æ‰§è¡Œæœ¬åœ°é€‚é…å™¨è·¯å¾„: {id_value}")
            data_text = adapter_func()
            
            if data_text:
                return data_text, id_value, alias
            else:
                print(f"æœ¬åœ°é€‚é…å™¨ {id_value} è·å–æ•°æ®å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢å› API (è‹¥æœ‰)...")
        url = f"{self.api_url}?id={id_value}&latest"

        proxies = None
        if self.proxy_url:
            proxies = {"http": self.proxy_url, "https": self.proxy_url}

        retries = 0
        while retries <= max_retries:
            try:
                response = requests.get(
                    url,
                    proxies=proxies,
                    headers=self.DEFAULT_HEADERS,
                    timeout=10,
                )
                response.raise_for_status()

                data_text = response.text
                data_json = json.loads(data_text)

                status = data_json.get("status", "æœªçŸ¥")
                if status not in ["success", "cache"]:
                    raise ValueError(f"å“åº”çŠ¶æ€å¼‚å¸¸: {status}")

                status_info = "æœ€æ–°æ•°æ®" if status == "success" else "ç¼“å­˜æ•°æ®"
                print(f"è·å– {id_value} æˆåŠŸï¼ˆ{status_info}ï¼‰")
                return data_text, id_value, alias

            except Exception as e:
                retries += 1
                if retries <= max_retries:
                    base_wait = random.uniform(min_retry_wait, max_retry_wait)
                    additional_wait = (retries - 1) * random.uniform(1, 2)
                    wait_time = base_wait + additional_wait
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}. {wait_time:.2f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}")
                    return None, id_value, alias

        return None, id_value, alias

    def crawl_websites(
        self,
        ids_list: List[Union[str, Tuple[str, str]]],
        request_interval: int = 100,
    ) -> Tuple[Dict, Dict, List]:
        """
        çˆ¬å–å¤šä¸ªç½‘ç«™æ•°æ®

        Args:
            ids_list: å¹³å°IDåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ– (å¹³å°ID, åˆ«å) å…ƒç»„
            request_interval: è¯·æ±‚é—´éš”ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            (ç»“æœå­—å…¸, IDåˆ°åç§°çš„æ˜ å°„, å¤±è´¥IDåˆ—è¡¨) å…ƒç»„
        """
        results = {}
        id_to_name = {}
        failed_ids = []

        for i, id_info in enumerate(ids_list):
            if isinstance(id_info, tuple):
                id_value, name = id_info
            else:
                id_value = id_info
                name = id_value

            id_to_name[id_value] = name
            response, _, _ = self.fetch_data(id_info)

            if response:
                try:
                    data = json.loads(response)
                    results[id_value] = {}
                    
                    # --- ä¿®æ”¹åçš„é€»è¾‘ ---
                    for index, item in enumerate(data.get("items", []), 1):
                        title = item.get("title")
                        # è·³è¿‡æ— æ•ˆæ ‡é¢˜ï¼ˆä¿æŒåŸæ ·ï¼‰
                        if title is None or isinstance(title, float) or not str(title).strip():
                            continue
                        
                        title = str(title).strip()
                        url = item.get("url", "")
                        mobile_url = item.get("mobileUrl", "")
                        # âœ¨ æ–°å¢ï¼šè·å–ä½ åœ¨ adapters.py ä¸­å®šä¹‰çš„æ—¥æœŸ/å‘å¸ƒæ—¶é—´
                        # å°è¯•è¯»å– date æˆ– release_time å­—æ®µ
                        date_val = item.get("date") or item.get("release_time") or ""
                    
                        # ğŸš« å…³é—­åˆå¹¶é€»è¾‘ï¼šä¸å†ä½¿ç”¨ title ä½œä¸º Keyï¼Œè€Œæ˜¯ä½¿ç”¨å¸¦åºå·çš„ Key
                        # è¿™æ ·å³ä½¿æ ‡é¢˜ä¸€æ¨¡ä¸€æ ·ï¼Œä¹Ÿä¼šå› ä¸ºåºå·ä¸åŒï¼ˆ001_, 002_...ï¼‰è€Œä½œä¸ºç‹¬ç«‹é¡¹ä¿å­˜
                        unique_key = f"{index:03d}_{title}" 
                    
                        results[id_value][unique_key] = {
                            "title": title,          # åŸå§‹æ ‡é¢˜
                            "ranks": [index],        # åŸå§‹æ’å
                            "url": url,
                            "mobileUrl": mobile_url,
                            "date": date_val         # âœ¨ ç¡®ä¿æ—¥æœŸè¢«å­˜å…¥ï¼Œä»¥ä¾¿åç»­é€šçŸ¥æ˜¾ç¤º
                        }
                except json.JSONDecodeError:
                    print(f"è§£æ {id_value} å“åº”å¤±è´¥")
                    failed_ids.append(id_value)
                except Exception as e:
                    print(f"å¤„ç† {id_value} æ•°æ®å‡ºé”™: {e}")
                    failed_ids.append(id_value)
            else:
                failed_ids.append(id_value)

            # è¯·æ±‚é—´éš”ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(ids_list) - 1:
                actual_interval = request_interval + random.randint(-10, 20)
                actual_interval = max(50, actual_interval)
                time.sleep(actual_interval / 1000)

        print(f"æˆåŠŸ: {list(results.keys())}, å¤±è´¥: {failed_ids}")
        return results, id_to_name, failed_ids
